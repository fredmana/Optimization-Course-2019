{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home assignment 1\n",
    "\n",
    "Asher Fredman<br>\n",
    "Shachar Schnapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Vector norms, matrix norms, and inner products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) For two matrices $A,B\\in \\mathbb{R}^{mxn}$, show that $\\langle A[:],B[:] \\rangle = trace(A^TB)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:<br>\n",
    "$\\langle A[:],B[:] \\rangle = \\sum_{j=1}^{n}\\sum_{i=1}^{m} A_{ij}\\times B_{ij} = trace(A^TB)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:<br>\n",
    "The first equation holds since the $A[:]$ operator just creates a vector stacking the $n$ columns of $A$, and by difinition of the regular vector dot product we sum over the multiplication of the corisponding $m\\times n$ entries in $A$ and $B$.<br>\n",
    "The second equation holds since the trace of $A^TB$ will return the sum of the entries where each entry is the dot product of each column in A with the corresponding column in B.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Find a $x_1$ vector that maximizes $\\ell_1$ and a $x_\\infty$ vector that maximizes $\\ell_\\infty$ for the matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A = \\begin{bmatrix}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "2 & 4 & -4 & 8 \\\\\n",
    "-5 & 4 & 1 & 5 \\\\\n",
    "5 & 0 & -3 & -7 \n",
    "\\end{bmatrix}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:<br>\n",
    "For the $x_1$, by definition of $\\begin{Vmatrix} A \\end{Vmatrix}_1$ we look for the column for which summing over the absolute value of it's entries returns the maximum value. for the given $A$ this will output $\\lvert 4 \\rvert + \\lvert 8 \\rvert + \\lvert 5 \\rvert + \\lvert -7 \\rvert = 24$.<br>\n",
    "For the $x_\\infty$, by definition of $\\begin{Vmatrix} A \\end{Vmatrix}_\\infty$ we look for the row for which summing over the absolute value of it's entries returns the maximum value. for the given $A$ this will output $\\lvert 2 \\rvert + \\lvert 4 \\rvert + \\lvert -4 \\rvert + \\lvert 8 \\rvert = 18$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Find the best approximation in a least square sense for satisfying the linear system $Ax = b$ where "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$A = \\begin{bmatrix}\n",
    "2 & 1 & 2 \\\\\n",
    "1 & -2 & 1 \\\\\n",
    "1 & 2 & 3 \\\\\n",
    "1 & 1 & 1 \n",
    "\\end{bmatrix}$ and $b = \\begin{bmatrix}\n",
    "6 \\\\\n",
    "1 \\\\\n",
    "5 \\\\\n",
    "2 \n",
    "\\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the normal equations and solve the problem using them. You may use a computer and provide the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:<br>\n",
    "The normal equation is given by $A^TAx = A^Tb$ and since A isn't singular we can take it's inverse and get $\\hat{x} = (A^TA)^{-1}A^Tb$.<br>Using the following code we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2,1,2],[1,-2,1],[1,2,3],[1,1,1,]])\n",
    "b = np.array([[6],[1],[5],[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATA = A.T.dot(A) \n",
    "ATA_inv = np.linalg.inv(ATA)\n",
    "ATb = A.T.dot(b)\n",
    "x = ATA_inv.dot(ATb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{x}$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.7]\n",
      " [0.6]\n",
      " [0.7]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Compute the residual of the least squares system $r = Ax − b$, with $x$ that you found in the previous section. Show that $A^⊤r = 0$. Is that surprising?\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = A.dot(x)-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The residual $(r)$ is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.00000000e-01]\n",
      " [ 2.00000000e-01]\n",
      " [-3.55271368e-15]\n",
      " [ 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we calculate $A^⊤r$ we get vector very close to the zero vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.24344979e-14]\n",
      " [-1.06581410e-14]\n",
      " [-1.95399252e-14]]\n"
     ]
    }
   ],
   "source": [
    "print(A.T.dot(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not suprising, since $A^⊤r$ is exactly the normal equation, which should equal zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Find the least squares solution of the system in (a), but now find a solution for which the first equation is almost exactly satisfied satisfied (let’s say, such that $\\lvert r_1 \\rvert < 10^{−3}$).\n",
    "\n",
    "Answer:\n",
    "We will solve the weighted least squares problem with the following <br><br>weight matrix $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1000.    0.    0.    0.]\n",
      " [   0.    1.    0.    0.]\n",
      " [   0.    0.    1.    0.]\n",
      " [   0.    0.    0.    1.]]\n"
     ]
    }
   ],
   "source": [
    "W = np.eye(4)\n",
    "W[0,0] = 1000\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATWA = A.T.dot(W).dot(A) \n",
    "ATWA_inv = np.linalg.inv(ATWA)\n",
    "ATWb = A.T.dot(W).dot(b)\n",
    "x_w = ATWA_inv.dot(ATWb)\n",
    "r_w = A.dot(x_w)-b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now get the following residual vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-8.07412821e-04]\n",
      " [ 2.69137606e-01]\n",
      " [-1.36424205e-12]\n",
      " [ 1.34568803e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(r_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Regularized Least Squares and SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $A \\in \\mathbb{R}^{mxn}$, we saw there exist orthogonal matrices $U$ and $V$, such that\n",
    "$A=UΣV^⊤$ (the SVD factorization of $A$), $Σ=diag(σ_1,σ_2,...,σ_p)$ $p=min(m,n)$,\n",
    "where $σ_1 ≥ σ_2 ≥ σ_3 ≥ ... ≥ σ_p ≥ 0$.<br>\n",
    "In our solutions we will shift between the using the full and economic versions of the SVD factorization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Using the normal equations and the SVD decompositon, show that the solution of the LS problem $argmin_x\\{∥Ax − b∥_2^2\\}$, is given by $\\hat{x} = (A^⊤A)^{−1}A^⊤b = VΣ^{−1}U^⊤b$.\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{x} = (A^⊤A)^{−1}A^⊤b$\n",
    "\n",
    "$A^⊤A = (UΣV^⊤)^⊤U\\Sigma V^⊤ = V\\Sigma^\\top U^\\top U\\Sigma V^\\top = V\\Sigma^\\top \\Sigma V^\\top$\n",
    "$A^⊤b = (UΣV^⊤)^\\top b = V\\Sigma^\\top U^\\top b$\n",
    "\n",
    "$\\Rightarrow \\hat{x} = \n",
    "(V\\Sigma^\\top \\Sigma V^\\top)^{-1}V\\Sigma^\\top U^\\top b = \n",
    "V\\Sigma^{-1}\\Sigma^{\\top^{-1}} V^{-1}V\\Sigma^\\top U^\\top b = \n",
    "V\\Sigma^{-1}\\Sigma^{\\top^{-1}}\\Sigma^\\top U^\\top b = \n",
    "VΣ^{−1}U^⊤b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Show that $V Σ^{−1}U^⊤ =  \\sum_{i=1}^{min(m,n)} σ^{−1}_iv_iu^⊤_i$ , where $u_i$ and $v_i$ are the columns of U and V respectively\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V Σ^{−1}U^⊤ = $\n",
    "\n",
    "$= \\begin{bmatrix}\n",
    "\\sigma_1^{-1}v_{11} & \\sigma_2^{-1}v_{12} & \\dots  & \\sigma_p^{-1}v_{1p} \\\\\n",
    "\\sigma_1^{-1}v_{21} & \\sigma_2^{-1}v_{22} & \\dots  & \\sigma_p^{-1}v_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma_1^{-1}v_{n1} & \\sigma_2^{-1}v_{n2} & \\dots  & \\sigma_p^{-1}v_{np}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "u_{11} & u_{12} & \\dots  & u_{1p} \\\\\n",
    "u_{21} & u_{22} & \\dots  & u_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "u_{m1} & u_{m2} & \\dots  & u_{mp}\n",
    "\\end{bmatrix}^\\top$\n",
    "\n",
    "$=\\begin{bmatrix}\n",
    "(\\sigma_1^{-1}v_{11}u_{11} + \\sigma_2^{-1}v_{12}u_{12} +\\dots+ \\sigma_p^{-1}v_{1p}u_{1p}) & \\dots & (\\sigma_1^{-1}v_{11}u_{m1} + \\sigma_2^{-1}v_{12}u_{m2} +\\dots+ \\sigma_p^{-1}v_{1p}u_{mp}) \\\\\n",
    "(\\sigma_1^{-1}v_{21}u_{11} + \\sigma_2^{-1}v_{22}u_{12} +\\dots+ \\sigma_p^{-1}v_{2p}u_{1p}) & \\dots & (\\sigma_1^{-1}v_{21}u_{m1} + \\sigma_2^{-1}v_{22}u_{m2} +\\dots+ \\sigma_p^{-1}v_{2p}u_{mp}) \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "(\\sigma_1^{-1}v_{n1}u_{11} + \\sigma_2^{-1}v_{n2}u_{12} +\\dots+ \\sigma_p^{-1}v_{np}u_{1p}) & \\dots & (\\sigma_1^{-1}v_{n1}u_{m1} + \\sigma_2^{-1}v_{n2}u_{m2} +\\dots+ \\sigma_p^{-1}v_{np}u_{mp}) \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$=\\begin{bmatrix}\n",
    "\\sigma_1^{-1}v_{11}u_{11} & \\dots & \\sigma_1^{-1}v_{11}u_{m1} \\\\\n",
    "\\sigma_1^{-1}v_{21}u_{11} & \\dots & \\sigma_1^{-1}v_{21}u_{m1} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma_1^{-1}v_{n1}u_{11} & \\dots & \\sigma_1^{-1}v_{n1}u_{m1} \\\\\n",
    "\\end{bmatrix}+\n",
    "\\begin{bmatrix}\n",
    "\\sigma_2^{-1}v_{12}u_{12} & \\dots & \\sigma_2^{-1}v_{12}u_{m2} \\\\\n",
    "\\sigma_2^{-1}v_{22}u_{12} & \\dots & \\sigma_2^{-1}v_{22}u_{m2} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma_2^{-1}v_{n2}u_{12} & \\dots & \\sigma_2^{-1}v_{n2}u_{m2} \\\\\n",
    "\\end{bmatrix}+ \\dots +\n",
    "\\begin{bmatrix}\n",
    "\\sigma_p^{-1}v_{1p}u_{1p} & \\dots & \\sigma_p^{-1}v_{1p}u_{mp} \\\\\n",
    "\\sigma_p^{-1}v_{2p}u_{1p} & \\dots & \\sigma_p^{-1}v_{2p}u_{mp} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma_p^{-1}v_{np}u_{1p} & \\dots & \\sigma_p^{-1}v_{np}u_{mp} \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$= σ^{−1}_1v_1u^⊤_1 + σ^{−1}_2v_2u^⊤_2 + \\dots σ^{−1}_pv_pu^⊤_p\n",
    "= \\sum_{n=1}^{min(m,n)} σ^{−1}v_iu^⊤_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Using the previous two sections show: \n",
    "$\\hat{x} = \\sum_{i=1}^{min(m,n)} \\frac{1}{σ_i}(u^⊤_ib)v_i$.<br><br>\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{x} = V Σ^{−1}U^⊤b = $\n",
    "\n",
    "$= \\begin{pmatrix} \\begin{bmatrix}\n",
    "v_{11} & v_{12} & \\dots  & v_{1p} \\\\\n",
    "v_{21} & v_{22} & \\dots  & v_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{n1} & v_{n2} & \\dots  & v_{np}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\sigma_1^{-1} & & \\\\\n",
    "& \\sigma_2^{-1} & & \\\\\n",
    "& & \\ddots & \\\\\n",
    "& & & \\sigma_p^{-1}\n",
    "\\end{bmatrix}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\begin{bmatrix}\n",
    "u_{11} & u_{12} & \\dots  & u_{1p} \\\\\n",
    "u_{21} & u_{22} & \\dots  & u_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "u_{m1} & u_{m2} & \\dots  & u_{mp}\n",
    "\\end{bmatrix}^\\top\n",
    "\\begin{bmatrix}\n",
    "    b_1 \\\\\n",
    "    b_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    b_n \n",
    "\\end{bmatrix}\n",
    "\\end{pmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\begin{bmatrix}\n",
    "\\sigma_1^{-1}v_{11} & \\sigma_2^{-1}v_{12} & \\dots  & \\sigma_p^{-1}v_{1p} \\\\\n",
    "\\sigma_1^{-1}v_{21} & \\sigma_2^{-1}v_{22} & \\dots  & \\sigma_p^{-1}v_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sigma_1^{-1}v_{n1} & \\sigma_2^{-1}v_{n2} & \\dots  & \\sigma_p^{-1}v_{np}\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "    u^⊤_1b \\\\\n",
    "    u^⊤_2b \\\\\n",
    "    \\vdots \\\\\n",
    "    u^⊤_pb\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\sum_{i=1}^{p}(\\frac{1}{σ_i}v_{1i}u^⊤_ib) \\\\\n",
    "    \\sum_{i=1}^{p}(\\frac{1}{σ_i}v_{2i}u^⊤_ib) \\\\\n",
    "    \\vdots \\\\\n",
    "    \\sum_{i=1}^{p}(\\frac{1}{σ_i}v_{pi}u^⊤_ib)\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$= \\frac{1}{σ_1}u^⊤_1b\\begin{bmatrix}\n",
    "    v_{11} \\\\\n",
    "    v_{21} \\\\\n",
    "    \\vdots \\\\\n",
    "    v_{n1}\n",
    "\\end{bmatrix} +\n",
    "\\frac{1}{σ_2}u^⊤_2b\\begin{bmatrix}\n",
    "    v_{12} \\\\\n",
    "    v_{22} \\\\\n",
    "    \\vdots \\\\\n",
    "    v_{n2}\n",
    "\\end{bmatrix} +\n",
    "\\frac{1}{σ_p}u^⊤_pb\\begin{bmatrix}\n",
    "    v_{1p} \\\\\n",
    "    v_{2p} \\\\\n",
    "    \\vdots \\\\\n",
    "    v_{np}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$= \\sum_{i=1}^{min(m,n)} \\frac{1}{σ_i}(u^⊤_ib)v_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) Consider a full SVD, where $U \\in \\mathbb{R}^{mxm}$ so we have $m$ vectors $u_i$ that span the whole $\\mathbb{R}^{m}$ space. Assume that $b =  \\sum_{i=1,..,m} α_iu_i$ for some $α_i$. Show that $\\hat{x} = \\sum_{i=1}^{min(m,n)} \\frac{1}{\\sigma_i}α_iv_i$.<br>\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the previous paragraph, it will be enough to show that for $i \\in \\{1,..,m\\},$ $u^⊤_ib = \\alpha_i$, and then placing $\\alpha_i$ in the previous result will give the desired answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$u^⊤_ib = u^⊤_i(\\sum_{j=1}^{m} \\alpha_ju_j) =$ \n",
    "$= \\alpha_1u^\\top_iu_1 + \\alpha_2u^\\top_iu_2 + \\dots + \\alpha_iu^\\top_iu_i + \\dots + \\alpha_mu^\\top_iu_m =$\n",
    "$= \\alpha_1\\times 0 + \\alpha_2\\times 0 + \\dots + \\alpha_i\\times 1 + \\dots + \\alpha_m\\times 0\n",
    "= \\alpha_i.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e) Show that the solution of the regularized LS problem $argmin_x\\{∥Ax − b∥_2^2 + \\lambda∥x∥_2^2\\}$, is given by the solution of the system $(A^⊤A + \\lambda I)x = A^⊤b$.\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll prove that the matrix $(A^⊤A + \\lambda I)$ is SPD for any $\\lambda > 0$.<br>\n",
    "Proof:<br>\n",
    "1) $A^⊤A$ is symetric: $(A^⊤A)^⊤ = A^⊤A^{⊤^⊤} = A^⊤A$.<br>\n",
    "2) $A^⊤A$ is PSD (i.e., $\\forall x\\neq 0$ $x^\\top A^⊤Ax \\geq 0$):<br>\n",
    "We'll look at the $\\ell_2$ norm of $Ax$. $ ∥ Ax ∥_2 = \\sqrt{(Ax)^⊤(Ax)} = \\sqrt{(x^⊤A^⊤Ax)} \\Rightarrow (x^⊤A^⊤Ax) \\geq 0$.<br>\n",
    "3) $\\lambda I$ is PD since it is diagonal with values $> 0$ on the diagonal and of course $x^⊤(\\lambda I)x = \\lambda x^⊤x_{(> 0)}$ for every $x>0$.<br>\n",
    "4) $x^⊤(A^⊤A + \\lambda I)x = x^⊤A^⊤Ax_{(\\geq 0)} + x^⊤(\\lambda I)x_{(>0)} > 0$. And so $(A^⊤A + \\lambda I)$ is PD.\n",
    "<br><br>\n",
    "Since $(A^⊤A + \\lambda I)$ is PD, than it is invertible (since all eigenvalues are greater than zero and the determenant is the multiplication of the eigen values and so $det(A^⊤A + \\lambda I) > 0$), than the solution of $(A^⊤A + \\lambda I)x = A^⊤b$ is $x = (A^⊤A + \\lambda I)^{-1}A^⊤b$.\n",
    "<br><br>\n",
    "We'll define $\\underset{(m+n)\\times n}{M}:=\\begin{bmatrix} A \\\\ \\sqrt{\\lambda}I \\end{bmatrix}$ \n",
    ", $\\underset{(m+n)\\times 1}{L}:=\\begin{bmatrix} b \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "$f(x) = \\hat{x} = argmin_x\\{∥Ax − b∥_2^2 + \\lambda∥x∥_2^2\\} = $\n",
    "\n",
    "$= argmin_x\\{∥Mx − L∥_2^2\\} = (Mx-L)^\\top(Mx-L) =$\n",
    "\n",
    "$= x^\\top M^\\top Mx - 2L^\\top Mx +L^\\top L$\n",
    "\n",
    "$\\nabla f(x) = 2M^\\top Mx - 2M^\\top L = $\n",
    "\n",
    "$= \\begin{pmatrix} \\begin{bmatrix} A \\\\ \\sqrt{\\lambda}I \\end{bmatrix}^\\top \\begin{bmatrix} A \\\\ \\sqrt{\\lambda}I \\end{bmatrix}\\end{pmatrix}^{-1}\\begin{bmatrix} A \\\\ \\sqrt{\\lambda}I \\end{bmatrix}^\\top\\begin{bmatrix} b \\\\ 0 \\end{bmatrix} =$\n",
    "\n",
    "$= (A^⊤A + \\lambda I)^{-1}A^⊤b$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f) Given the previous sections, show that the solution of the regularized LS problem is given by \n",
    "\n",
    "$\\sum_{i=1}^{min(m,n)} \\frac{σ_i}{σ^2_i+\\lambda}\\alpha_iv_i$.\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{x} = \n",
    "(V\\Sigma^\\top \\Sigma V^\\top + \\lambda I)^{-1}V\\Sigma^\\top U^\\top b$\n",
    "\n",
    "$\\Rightarrow V^{-1}\\hat{x} = \n",
    "(V\\Sigma^2 + \\lambda IV)^{-1}V\\Sigma^\\top U^\\top b = (V\\Sigma^2 + V\\lambda I)^{-1}V\\Sigma^\\top U^\\top b$\n",
    "\n",
    "$\\Rightarrow V^{-1}\\hat{x} = \n",
    "(\\Sigma^2 + \\lambda I)^{-1}V^{-1}V\\Sigma^\\top U^\\top b = (\\Sigma^2 + \\lambda I)^{-1}\\Sigma^\\top U^\\top b$\n",
    "\n",
    "$\\Rightarrow \\hat{x} = \n",
    "V(\\Sigma^2 + \\lambda I)^{-1}\\Sigma^\\top U^\\top b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $(\\Sigma^2 + \\lambda I)^{-1}\\Sigma^\\top$ is just $diag(\\frac{σ_i}{σ^2_i+\\lambda})$ for $i \\in [1,\\dots ,p]$\n",
    "Then we get:<br>\n",
    "$\\hat{x} = \n",
    "V(\\Sigma^2 + \\lambda I)^{-1}\\Sigma^\\top U^\\top b = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= \\begin{pmatrix} \\begin{bmatrix}\n",
    "v_{11} & v_{12} & \\dots  & v_{1p} \\\\\n",
    "v_{21} & v_{22} & \\dots  & v_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "v_{n1} & v_{n2} & \\dots  & v_{np}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{σ_1}{σ^2_1+\\lambda} & & \\\\\n",
    "& \\frac{σ_2}{σ^2_2+\\lambda} & & \\\\\n",
    "& & \\ddots & \\\\\n",
    "& & & \\frac{σ_p}{σ^2_p+\\lambda}\n",
    "\\end{bmatrix}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\begin{bmatrix}\n",
    "u_{11} & u_{12} & \\dots  & u_{1p} \\\\\n",
    "u_{21} & u_{22} & \\dots  & u_{2p} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "u_{m1} & u_{m2} & \\dots  & u_{mp}\n",
    "\\end{bmatrix}^\\top\n",
    "\\begin{bmatrix}\n",
    "    b_1 \\\\\n",
    "    b_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    b_n \n",
    "\\end{bmatrix}\n",
    "\\end{pmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And picking on answers to previous sections we obtaion that this results in $\\sum_{i=1}^{min(m,n)} \\frac{σ_i}{σ^2_i+\\lambda}\\alpha_iv_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (g) Try to explain the deblurring example using the results above. Assume that the noise in the image corresponds to a singular vector $u_i$ with a small corresponding singular value $σ_i$.<br>\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we found that $\\hat{x} = \\sum_{i=1}^{min(m,n)} \\frac{1}{\\sigma_i}α_iv_i$ for the regular case, and $\\hat{x} = \\sum_{i=1}^{min(m,n)} \\frac{σ_i}{σ^2_i+\\lambda}\\alpha_iv_i$ for the regularized case, we can assume the reason that for the ragular case, very small values of $σ_i$ might produce an amplified version of the noise, and thus producing a completely noisy image as shown in the example. On the other hand when adding the $\\lambda$ regularization term, and thus prevening this 'explosion'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Camera Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let  $K = \\begin{bmatrix}\n",
    "    f_x & 0 & x_0 \\\\\n",
    "    0 & f_y & y_0 \\\\\n",
    "    0 & 0 & 1\n",
    "\\end{bmatrix}$, note that $b = K\\begin{bmatrix}\n",
    "    x \\\\\n",
    "    y \\\\\n",
    "    z\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    x \\cdot f_x + z \\cdot x_0 \\\\\n",
    "    y\\cdot f_y + y_0 \\cdot z \\\\\n",
    "    z\n",
    "\\end{bmatrix}$. therefore $ \\begin{bmatrix}\n",
    "    u \\\\\n",
    "    v\n",
    "\\end{bmatrix}$ = $ \\begin{bmatrix}\n",
    "    x \\cdot f_x + z \\cdot x_0 \\\\\n",
    "    y\\cdot f_y + y_0 \\cdot z\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As disscused with the TA, we ignore the last entry representing the 3rd dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) What is the minimal number of correspondences required to find K?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we have 4 parmters ($f_x, x_0, y_0, f_y$), we have a solution if the rank of the samples is up to 2 (each correspondent gives us two equations), so if we have 2 (independent) samples we can solve this problme using Gaussian elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Assuming the number of correspondences, n, is larger than the minimum number you found in the previous section, how would you solve the problem to obtain a good solution for K?<br>Show all your computations and derivations for the optimal solution in a least squares sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the rank of the samples more then 4, we dont have solution for this problem. we use LSM \n",
    "we mark the error of the LSM as:\n",
    "<br>\n",
    "$ Err(𝑓_𝑥, x_0, f_y, y_0) = \\sum_{i=1}^n || (u_i, v_i) - (x_i \\cdot f_x + z_i \\cdot x_0,  y_i\\cdot f_y + y_0 \\cdot z_i) ||^2_2 $ = $\\sum_{i=1}^n (u_i - x_i \\cdot f_x + z_i \\cdot x_0) ^2 + (v_i -  y_i\\cdot f_y + y_0 \\cdot z_i)^2$\n",
    "<br>\n",
    "wherr $n$ is the number of the smaples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to find $𝑓_𝑥^*, x_0^*, f_y^*, y_0^*$ = argmin$_{𝑓_𝑥, x_0, f_y, y_0} Err(𝑓_𝑥, x_0, f_y, y_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we compute the derivative and find $Err$ minimum: \n",
    "<br>\n",
    "$\\frac{\\partial Err}{\\partial f_x} = -\\sum_{i=1}^n 2(u_i - f_xx_i - z_ix_0) x_i = 2(\\sum_{i=1}^n u_ix_i - f_x\\sum_{i=1}^n x_i^2 - x_0\\sum_{i=1}^n z_ix_i )$ \n",
    "<br>\n",
    "$\\frac{\\partial Err}{\\partial x_0} = -\\sum_{i=1}^n 2(u_i - f_xx_i - z_ix_0) z_i = 2(\\sum_{i=1}^n u_iz_i - f_x\\sum_{i=1}^n z_ix_i - x_0\\sum_{i=1}^n z_i^2)$ \n",
    "<br>\n",
    "$\\frac{\\partial Err}{\\partial f_y} = -\\sum_{i=1}^n 2(v_i - f_yy_i - z_iy_0) y_i = 2(\\sum_{i=1}^n v_iy_i - f_y\\sum_{i=1}^n y_i^2 - y_0\\sum_{i=1}^n z_iy_i)$\n",
    "<br>\n",
    "$\\frac{\\partial Err}{\\partial y_0} = -\\sum_{i=1}^n 2(v_i - f_yy_i - z_iy_0) z_i = 2(\\sum_{i=1}^n v_iz_i - f_y\\sum_{i=1}^n z_iy_i - y_0\\sum_{i=1}^n z_i^2)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note:\n",
    "<br>\n",
    "$A = \\sum_{i=1}^n x_i^2, B = \\sum_{i=1}^n z_ix_i, C = \\sum_{i=1}^n u_ix_i, D = \\sum_{i=1}^n z_i^2, E=\\sum_{i=1}^n z_iu_i$\n",
    "<br>\n",
    "$F = \\sum_{i=1}^n v_iy_i, G = \\sum_{i=1}^n y_i^2, H = \\sum_{i=1}^n z_iy_i, I = \\sum_{i=1}^n v_iz_i$\n",
    "<br>\n",
    "Therefore the normal equations are:\n",
    "<br>\n",
    "(1) $ Af_x + Bx_0 = C $\n",
    "<br>\n",
    "(2) $ Bf_x + Dx_0 = E $\n",
    "<br>\n",
    "(3) $ Gf_y + Hy_0 = F$\n",
    "<br>\n",
    "(4) $ Hf_y + Dy_0 = I $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the rseults are:\n",
    "<br>\n",
    "    $x_0 = \\frac{EA}{B^2 + C + DA}$   \n",
    "<br>\n",
    "    $f_x = \\frac{Bx_0 + C}{A}$\n",
    "<br>\n",
    "    $y_ 0 = \\frac{IG}{H^2 + F + DG}$  \n",
    "<br>\n",
    "    $f_y =  \\frac{Hy_0 + F}{G}$    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
