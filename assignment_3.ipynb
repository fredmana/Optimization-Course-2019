{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asher Fredman and Shahar Schnapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_sparse_spd_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Equality constrained optimization\n",
    "In this section we will find the maximal surface area of a box given the sum of its edges length. <br>\n",
    "The optimization problem is given by: <br>\n",
    "$max_{x \\in R^3}\\{x_1x_2 +x_2x_3 +x_1x_3\\}$  $s.t.$  $x_1+x_2+x_3=3 $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Find a critical point for the problem using the Lagrange multiplier method.\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(x, \\lambda) = f(x) + \\lambda c^{eq}(x)\n",
    "= x_1x_2 +x_2x_3 +x_1x_3 + \\lambda (x_1+x_2+x_3-3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We require that $\\nabla L(x, \\lambda) =  0$,\n",
    "meaning $\\nabla L(x, \\lambda) = \n",
    "\\begin{bmatrix}\n",
    "x_2 + x_3 + λ \\\\\n",
    "x_1 + x_3 + λ \\\\\n",
    "x_1 + x_2 + λ \\\\\n",
    "x_1 + x_2 +x_3 -3 \n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \n",
    "\\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 4 equations with 4 unnowns, for which the solution is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [-2.]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[0,1,1,1],[1,0,1,1],[1,1,0,1],[1,1,1,0]])\n",
    "b = np.array([[0],[0],[0],[3]])\n",
    "print(np.linalg.solve(A, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get $x_1=x_2=x_3=1$, and  $λ=-2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Show that this critical point is a maximum point.\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_x^2L = \n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 1 & 0 \n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the critical point to be a maximum point, we need to show that $y^\\top \\nabla_x^2L y < 0$ for vectors $y \\neq 0$ who satisfy $y^⊤1 = y_1 + y_2 + y_3 = 0$.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y^\\top \\nabla_x^2L y = \n",
    "\\begin{bmatrix}\n",
    "y_2+y_3 & y_1+y_3 & y_1+y_2 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3 \n",
    "\\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $y^⊤1 = y_1 + y_2 + y_3 = 0$, then we have that:<br>\n",
    "$y_2 + y_3 = -y_1$<br>\n",
    "$y_1 + y_3 = -y_2$<br>\n",
    "$y_1 + y_2 = -y_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow y^\\top \\nabla_x^2L y = \n",
    "\\begin{bmatrix}\n",
    "-y_1 & -y_2 & -y_3 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "y_3 \n",
    "\\end{bmatrix} = -(y_1^2 + y_2^2 + y_3^2) < 0$ for any $y \\neq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. General constrained optimization\n",
    "Assume that we have the following problem:<br><br>\n",
    "$min_{x \\in R^2}\\{(x_1+x_2)^2 -10(x_1+x_2)\\}  \n",
    "s.t.  \\begin{cases}\n",
    "  3x_1 + x_2 = 6\\\\    \n",
    "  x_1^2 + x_2^2 \\leq 5\\\\  \n",
    "  -x_1 \\leq 0\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Find a critical point $x^∗$ using the Lagrange multipliers method for which the fewest inequality constraints are active. Show that the KKT conditions hold. (no need to determine whether this point is a minimum or maximum).\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define:<br><br>\n",
    "$x:=[x_1, x_2]^\\top$<br><br>\n",
    "$f(x) = (x_1+x_2)^2 -10(x_1+x_2)$<br><br>\n",
    "$c^{eq} := [3x_1 + x_2 - 6 = 0]$<br><br>\n",
    "$c^{ieq} := \n",
    "\\begin{bmatrix}\n",
    "x_1^2 + x_2^2 -5 \\leq 0 \\\\\n",
    "-x_1 \\leq 0\n",
    "\\end{bmatrix}$<br><br>\n",
    "Where as we learned, the lagrangian for this case is:<br><br>\n",
    "$L(x, λ^{eq} , λ^{ieq}) = f(x) + (λ^{eq})^⊤ c^{eq}(x) + (λ^{ieq})^⊤c^{ieq}(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the KKT conditions to hold we need to show that for a given local solution $x^*$ the following conditions are satisfied:<br>\n",
    "1. $\\nabla_x L(x^*, λ^{eq^*} , λ^{ieq^*}) = 0$<br>\n",
    "2. $c^{eq}(x^*) = 0$<br>\n",
    "3. $c^{ieq}(x^*) \\leq 0$<br>\n",
    "4. $λ^{ieq^*} \\geq 0$<br>\n",
    "5. $λ_l^{ieq^*}c^{ieq}(x^*) = 0$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now go over all the combinations of possibly active $c^{ieq}$'s which satisy the KKT conditions, and we will start with the smallest combination possible.<br>\n",
    "As we learned for the active $c^{ieq}$'s condition for is $λ^{ieq^*} > 0$ and for the non-active ones it's $λ^{ieq^*} = 0$ (so we can ignore the non-active c's)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### option 1: there are no active $c^{ieq}$'s.\n",
    "In that case:<br><br>\n",
    "$L(x, λ^{eq} , λ^{ieq})$<br> \n",
    "$=f(x) + (λ^{eq})^⊤ c^{eq}(x) + (0)^⊤c^{ieq}(x)$<br>\n",
    "$=(x_1+x_2)^2 -10(x_1+x_2) + (λ_1^{eq})^⊤(3x_1 + x_2 - 6) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla L(x, \\lambda) = \n",
    "\\begin{bmatrix}\n",
    "2x_1 + 2x_2 -10 + 3λ_1 \\\\\n",
    "2x_1 + 2x_2 -10 + λ_1 \\\\\n",
    "3x_1 + x_2 -6\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \n",
    "\\end{bmatrix}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 3 equations with 3 unnowns, for which the solution is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5]\n",
      " [4.5]\n",
      " [0. ]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[2,2,3],[2,2,1],[3,1,0]])\n",
    "b = np.array([[10],[10],[6]])\n",
    "print(np.linalg.solve(A, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we get $x_1=0.5, x_2=4.5$, and  $λ_1=0$.\n",
    "Placing these values into $c^{eq}$ we see that the equality does not hold, and therefor the KKT conditions are not satisfied for this combination:<br>\n",
    "$c^{eq}(x) = 1.5-4.5-6 = -9 \\neq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### option 2: there is one active $c^{ieq}$.\n",
    "Let's assume that only the first $c^{ieq}$ is active (and so $λ^{ieq} = [λ_2,0]^\\top$):<br><br>\n",
    "$L(x, λ^{eq} , λ^{ieq})=f(x) + (λ^{eq})^⊤ c^{eq}(x) + (0)^⊤c^{ieq}(x)$<br>\n",
    "$=(x_1+x_2)^2 -10(x_1+x_2) + (λ_1^{eq})^⊤(3x_1 + x_2 - 6) + (λ_2^{ieq})^⊤(x_1^2 + x_2^2 -5)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla L(x, \\lambda) = \n",
    "\\begin{bmatrix}\n",
    "2x_1 + 2x_2 -10 + 3λ_1 + 2λ_2x_1\\\\\n",
    "2x_1 + 2x_2 -10 + λ_1 + 2λ_2x_2\\\\\n",
    "3x_1 + x_2 -6 \\\\\n",
    "x_1^2 + x_2^2 -5\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first find $x_1$ and $x_2$ using the last two equations.<br><br>\n",
    "$x_2 = 6-3x_1$<br>\n",
    "$x_1^2 + (6-3x_1)^2 = 5$<br>\n",
    "$x_1^2 + 9x_1^2 + 36 -36x_1 = 5$<br>\n",
    "$10x_1^2 - 36x_1 +31 = 0$<br><br>\n",
    "Solving the above quedratic equation we get two possible solutions:<br>\n",
    "$x_{1_a,2_a} = [2.174, -0.522]$<br>\n",
    "$x_{1_b,2_b} = [1.426, 1.722]$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can easily be validated that both solutions satisfy the $c^{eq}$ constrained. Now we move on the find the λ's.<br><br>\n",
    "Using the first two equations with $x_{1_a,2_a}$ we get:<br>\n",
    "1. $λ_{2_a} = \n",
    "\\frac{5-x_2}{x_1} - 1 - \\frac{1.5λ_{1_a}}{x_1} = \n",
    "1.54-0.6λ_{1_a}$<br>\n",
    "2. $λ_{2_a} = \n",
    "\\frac{5-x_1}{x_2} - 1 - \\frac{0.5λ_{1_a}}{x_2} = \n",
    "-6.41+0.9λ_{1_a}$<br>\n",
    "$\\Rightarrow 1.5λ_{1_a} = 7.95\n",
    "\\Rightarrow λ_{1_a,2_a} = [5.3, -1.64]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $λ_{2_a} < 0$ which does not satisfy the 4TH KKT condition, so we move on the check the values of λ using $x_{1_b,2_b}$:<br>\n",
    "1. $λ_{2_b} = \n",
    "\\frac{5-x_2}{x_1} - 1 - \\frac{1.5λ_{1_b}}{x_1} = \n",
    "0.3 - 0.07λ_{1_a}$<br>\n",
    "2. $λ_{2_b} = \n",
    "\\frac{5-x_1}{x_2} - 1 - \\frac{0.5λ_{1_b}}{x_2} = \n",
    "1.135-0.29λ_{1_b}$<br>\n",
    "$\\Rightarrow 0.22λ_{1_b} = 0.835\n",
    "\\Rightarrow λ_{1_b,2_b} = [3.8, 0.035]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are left to validate that the 5TH KKT condition holds using the $λ^{ieq}$ and the $x$ we found, and indeed:<br>\n",
    "$c^{ieq}[2](x) = x_1^2 + x_2^2 -5 =\n",
    "1.426^2 + 1.722^2 - 5 = 0$<br>\n",
    "(for the non-active $c^{ieq}$ we take $λ=0$ and it is immediate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Write the unconstrained minimization problem that corresponds to the problem above using the penalty method with $ρ(x) = x^2$ as a penalty function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find $min_{x \\in R^n}\\{f_\\mu(x)\\}$ where<br> \n",
    "$f_\\mu(x) := (x_1+x_2)^2 -10(x_1+x_2) + \n",
    "\\mu\\times[(3x_1 + x_2 - 6)^2 + (max\\{0, x_1^2 + x_2^2 - 5\\})^2 + (max\\{0, -x_1\\})^2]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Use the penalty method to get the minimizer $x^∗$ up to two digits of accuracy. Solve the optimization problems using steepest descent with Armijo linesearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : (x[0]+x[1])**2 -10*(x[0]+x[1])\n",
    "\n",
    "def get_f_mu(mu):\n",
    "    return lambda x,f,c : f(x) + mu*((c[0](x))**2 + max(0,c[1](x))**2 + max(0,c[2](x))**2)\n",
    "\n",
    "def get_constraintes():\n",
    "    return [lambda x : 3*x[0] + x[1] -6, \n",
    "            lambda x : x[0]**2 + x[1]**2 -5, \n",
    "            lambda x : -x[0]]\n",
    "\n",
    "def get_g_mu(mu):\n",
    "    return [lambda x,c : 2*(x[0]+x[1]) - 10 + 6*mu * (c[0](x)) + mu*2*x[0] * (max(0, c[1](x)) * c[1](x)) -mu * (max(0, c[2](x)) * c[2](x)), \n",
    "            lambda x,c : 2*(x[0]+x[1]) - 10 + 2*mu * (c[0](x)) + mu*2*x[1] * (max(0, c[1](x)) * c[1](x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linesearch(f_mu,x,f,constraintes,d,gk,alpha0,beta,c):\n",
    "    alphaj = alpha0\n",
    "    for jj in range(20):\n",
    "        x_temp = np.ndarray.tolist(np.array(x) + alphaj*d)\n",
    "        if f_mu(x_temp,f,constraintes) <= f_mu(x,f,constraintes) + alphaj*c*np.dot(d,gk):\n",
    "            break\n",
    "        else:\n",
    "            alphaj = alphaj*beta\n",
    "    return alphaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.41  1.758]\n"
     ]
    }
   ],
   "source": [
    "mus = [0.01, 0.1, 1, 10, 100]\n",
    "beta = 0.5\n",
    "c = 1e-4\n",
    "alpha_prev = 1.0\n",
    "x_SD = [-1.4, 2.0]\n",
    "constraintes = get_constraintes()\n",
    "\n",
    "for mu in mus:\n",
    "    f_mu = get_f_mu(mu)\n",
    "    g_mu = get_g_mu(mu)\n",
    "    for k in range(500):\n",
    "        gk = np.array([g_mu[0](x_SD, constraintes), g_mu[1](x_SD, constraintes)])\n",
    "        d_SD = -gk\n",
    "        alpha_SD = linesearch(f_mu,x_SD,f,constraintes,d_SD,gk,alpha_prev,beta,c)\n",
    "        x_SD=np.ndarray.tolist(np.array(x_SD)+alpha_SD*d_SD)\n",
    "print(np.around(x_SD, decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when solvin the optimization problem using steepest descent with Armijo linesearch we get:<br>\n",
    "$x_{1,2} = [1.41, 1.758]$ which is pretty close to the $[1.426, 1.722]$ we got before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Box-constrained optimization\n",
    "In this question we will write the projected coordinate descent method for quadratic box-constrained minimization. Assume the following problem<br><br>\n",
    "$min_{x \\in R^n}\\{\\frac{1}{2}x^\\top Hx - x^\\top g\\}$  s.t. $a\\leq x \\leq b$,<br><br>\n",
    "where $H ∈ R^{n×n}$ is symmetric positive definite, $g ∈ R^n$, and $a < b ∈ R^n$ are the lower and upper bounds on the solution x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Give a closed form solution for the scalar box constrained minimization problem $min_{x \\in R}\\{\\frac{1}{2}hx^2 - gx\\}$ s.t. $a\\leq x \\leq b$, for $a < b, h > 0$. \n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $f(x):=\\frac{1}{2}hx^2 - gx$ and $c^{ieq} := \n",
    "\\begin{bmatrix}\n",
    "a - x \\leq 0 \\\\\n",
    "x - b \\leq 0\n",
    "\\end{bmatrix}$<br><br>\n",
    "Let us define the following lagrangian function:<br>\n",
    "$L(x, \\lambda) = \n",
    "\\frac{1}{2}hx^2 - gx + \\lambda_1(a - x) + \\lambda_2(x - b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_x L(x, \\lambda) = \n",
    "\\begin{bmatrix}\n",
    "hx - g - λ_1 + λ_2\n",
    "\\end{bmatrix}$.<br><br>\n",
    "$hx - g - λ_1 + λ_2 = 0 \\Rightarrow x = \\frac{g + λ_1 - λ_2}{h}$<br><br>\n",
    "This point is a minumim point since $\\nabla_x^2 L(x, \\lambda) = h > 0$.<br><br>\n",
    "We'll define $z:= \\frac{g + λ_1 - λ_2}{h}$, which gives us:<br><br>\n",
    "$x^* = \\begin{cases}\n",
    "  z & \\text{if }a\\le z\\le b\\\\ \\\\    \n",
    "  a & \\text{if }z < a\\\\ \\\\  \n",
    "  b & \\text{if }z > b\\\\\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that in order to find $λ_1$ and $λ_2$ we can slipt the possible values of $x$ into three groups:<br>\n",
    "(1) $a\\le z\\le b$. For this case since $x$ is in a valid domain then both $λ_1,λ_2 = 0$ and so $z = \\frac{g}{h}$<br>\n",
    "(2) $z < a$. Since we also know that $a<b$, then in this case ovbiously the second inequality in $c^{ieq}$ ($x - b \\leq 0$) is true and therefor $λ_2 = 0$.<br>\n",
    "We get two equations: $x = \\frac{g + λ_1}{h}$ and $x = a$, which means that $λ_1 = a\\times h - g$.<br>\n",
    "Since we know that $z = \\frac{g}{h} < a$ then $λ_1 = a\\times h - g > \\frac{g}{h}\\times h - g = 0$ and so $λ_1 \\geq 0$.<br>\n",
    "(3) $b < x$. This case is very similar to the second case and so $λ_2 = -b\\times h + g$.<br>\n",
    "Now since we know that $z = \\frac{g}{h} > b$ then as before we would get that $λ_1 \\geq 0$).<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) In the projected coordinate descent algorithm we sweep over all the variables $x_i$ one by one, and for each solve the box-constrained minimization problem for the scalar variable $x_i$, given that the rest are known. Show that the minimization for each scalar $x_i$ is given by\n",
    "$min_{x_i \\in R}\\{\\frac{1}{2}h_{ii}x_i^2 + [(\\sum_{j\\neq i}h_{ij}x_j) - g_i]x_i\\}$ s.t. $a_i\\leq x_i \\leq b_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that all $x_j$'s are known, where $j\\neq i$, we are just left with minimizing $\\frac{1}{2}h_{ii}x_i^2 + [(\\sum_{j\\neq i}h_{ij}x_j) - g_i]x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) $\\frac{1}{2}x^\\top Hx = \n",
    "\\frac{1}{2}\\sum_{i=1}^{n}x_i\\sum_{j=1}^{n}x_jH_{j} = \n",
    "\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{i=1}^{n}x_ix_jH_{ji} =$\n",
    "$= \\frac{1}{2}\\sum_{i=1}^{n}(x_i^2H_{ii} + \\sum_{i\\neq j}x_ix_jH_{ji})$<br><br>\n",
    "(2) $x^\\top g = \\sum_{i=1}^{n}x_ig_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $f(x):= \\frac{1}{2}x^\\top Hx - x^\\top g$ and since H is symetric ($h_{ij} = h_{ji}$) we have that:<br><br>\n",
    "(1 + 2) $\\Rightarrow f(x) = \\sum_{i=1}^{n} \\frac{1}{2}h_{ii}x_i^2 + [(\\sum_{j\\neq i}h_{ij}x_j) - g_i]x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)+(d) Use the previous section to write a program for solving the using a projected coordinate descent algorithm, and solve it for the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:\n",
      "[[ 5. -1. -1. -1. -1.]\n",
      " [-1.  5. -1. -1. -1.]\n",
      " [-1. -1.  5. -1. -1.]\n",
      " [-1. -1. -1.  5. -1.]\n",
      " [-1. -1. -1. -1.  5.]]\n",
      "g:\n",
      "[[ 18.]\n",
      " [  6.]\n",
      " [-12.]\n",
      " [ -6.]\n",
      " [ 18.]]\n",
      "a:\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "b:\n",
      "[[5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]\n",
      " [5.]]\n"
     ]
    }
   ],
   "source": [
    "H = -np.ones((5,5))\n",
    "for i in range(5):\n",
    "    H[i,i] = 5\n",
    "a = np.zeros((5,1))\n",
    "b = 5*np.ones((5,1))\n",
    "x_initial = 2.5*np.ones((5,1))\n",
    "g = np.expand_dims(np.array([18.,6.,-12.,-6.,18.]),1)\n",
    "print(\"H:\")\n",
    "print(H)\n",
    "print(\"g:\")\n",
    "print(g)\n",
    "print(\"a:\")\n",
    "print(a)\n",
    "print(\"b:\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though it was not required, In addition to the  the coordinate descent solution (which we will show now), we decided to compare the results with the projected steepest descent algorithm.<br><br>\n",
    "(1) Coordinate descent solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project (x_ind, bottom, top):\n",
    "    return min(max(x_ind,bottom),top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_ind(x, i, H, g):\n",
    "    mult_sum = 0\n",
    "    for j in range(H.shape[0]):\n",
    "        if j != i:\n",
    "            mult_sum += H[i][j] * x[j][0]\n",
    "    x_sol =  -(mult_sum - g[i][0]) / H[i][i]\n",
    "    return project(x_sol, a[i][0], b[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_cd(H, g, x_initial, maxIter, convergence_criterion):\n",
    "    x_sol = x_initial\n",
    "    x_l = x_sol.shape[0]\n",
    "    for itera in range(maxIter):\n",
    "        for i in range(x_l):\n",
    "            x_sol[i] = proj_ind(x_sol, i, H, g)\n",
    "    return x_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxIter = 100\n",
    "convergence_criterion = 0.001\n",
    "learned_x = proj_cd(H, g, x_initial, maxIter, convergence_criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.   ]\n",
      " [3.667]\n",
      " [0.667]\n",
      " [1.667]\n",
      " [5.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.around(learned_x, decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Comparing the coordinate descent solution with a projected steepest descent solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_proj = lambda x,H,g : 0.5*(x.T.dot(H)).dot(x) - x.T.dot(g)\n",
    "g_proj = lambda x,H,g : H.dot(x) - g\n",
    "\n",
    "def proj_line_search(f,x,H,g,d,gk,alpha0,beta,c):\n",
    "    alphaj = alpha0\n",
    "    for jj in range(10):\n",
    "        x_temp = x + alphaj*d\n",
    "        curr_val = f_proj(x_temp,H,g)\n",
    "        prev_val = f_proj(x,H,g)\n",
    "        res = alphaj*c*(d.T.dot(gk))\n",
    "        if curr_val <= prev_val + res:\n",
    "            break\n",
    "        else:\n",
    "            alphaj = alphaj*beta\n",
    "    return alphaj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_sd(f_proj, g_proj, H, g, x_0, maxIter, convergence_criterion):\n",
    "    x_prev = x_0\n",
    "    convergence_prev = np.linalg.norm(f_proj(x_prev,H,g))\n",
    "    for k in range(maxIter):\n",
    "        g_prev = g_proj(x_prev,H,g)\n",
    "        d = -g_prev\n",
    "        alpha = proj_line_search(f,x_prev,H,g,d,g_prev,alpha_prev,beta,c)\n",
    "        z_k = x_prev - alpha * g_prev\n",
    "        x_k = np.minimum(np.maximum(a,z_k), b)\n",
    "        convergence = np.linalg.norm(f_proj(x_k,H,g))\n",
    "        convergence_factor = convergence / convergence_prev\n",
    "        if convergence_factor <= convergence_criterion:\n",
    "            break\n",
    "        x_prev = x_k\n",
    "        convergence_prev = convergence\n",
    "    return x_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxIter = 100\n",
    "convergence_criterion = 0.001\n",
    "learned_x = proj_sd(f_proj, g_proj, H, g, x_initial, maxIter, convergence_criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.   ]\n",
      " [3.667]\n",
      " [0.667]\n",
      " [1.667]\n",
      " [5.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.around(learned_x, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
